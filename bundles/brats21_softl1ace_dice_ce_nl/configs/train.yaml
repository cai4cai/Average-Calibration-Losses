# configuration common to all training runs

num_epochs: 600  # default value that can be overridden in data config
batch_size: 2
cache_size: 256
ckpt_interval: 1
num_workers: 16
num_substeps: 1

##########################
# OPTIMIZER
##########################
learning_rate: 0.0001
num_warmup_epochs: 2

optimizer:
  _target_: torch.optim.AdamW
  params: $@network.parameters()
  lr: '@learning_rate'
  weight_decay: 1.0e-05

##########################
# LEARNING RATE SCHEDULER
##########################
lr_scheduler:
  _target_: WarmupCosineSchedule
  optimizer: '@optimizer'
  warmup_steps: '@num_warmup_epochs'
  warmup_multiplier: 0.50
  t_total: '@num_epochs'

##########################
# DATASETS
##########################

train_dataset:
  _target_: CacheDataset
  data: '@train_sub'
  transform: '@train_transforms'
  cache_num: '@cache_size'
  num_workers: '@num_workers'


##########################
# DATALOADERS
##########################
train_dataloader:
  _target_: ThreadDataLoader  # generate data ansynchronously from training
  dataset: '@train_dataset'
  batch_size: '@batch_size'
  repeats: '@num_substeps'
  num_workers: '@num_workers'


##########################
# TRAINER CONFIGURATION
##########################
training_handlers:
- '@metriclogger'
- _target_: LrScheduleHandler
  lr_scheduler: '@lr_scheduler'
- _target_: ValidationHandler  # run validation at the set interval, bridge between trainer and evaluator objects
  validator: '@evaluator'
  epoch_level: true
  interval: '@val_interval'
- _target_: StatsHandler
  name: null  # use engine.logger as the Logger object to log to
  tag_name: train_loss
  output_transform: $monai.handlers.from_engine(['loss'], first=True)  # log loss value
- _target_: LogfileHandler  # log outputs from the training engine
  output_dir: '@output_dir'
- _target_: TensorBoardStatsHandler
  summary_writer: '@writer'
  tag_name: train
  output_transform: $monai.handlers.from_engine(['loss'], first=True)

deep_supervised_loss_fn:
  _target_: DeepSupervisionLoss
  loss: '@lossfn'

postprocessing: null  # can be overwritten in data config, for example for HEC training

trainer:
  _target_: SupervisedTrainer
  max_epochs: '@num_epochs'
  device: '@device'
  train_data_loader: '@train_dataloader'
  network: '@network'
  loss_function: '@deep_supervised_loss_fn'
  optimizer: '@optimizer'
  key_train_metric: null
  postprocessing: '@postprocessing'
  train_handlers: '@training_handlers'
  amp: '@amp'


##########################
# CHECKPOINTING
##########################
# Get latest checkpoint, if it exists:
ckpt_path: '$max(glob.glob(os.path.join(@output_dir, "ckpt_checkpoint_epoch=*.pt")), key=lambda f: int(os.path.basename(f).split("=")[1].split(".")[0]), default="none")'

# Create a checkpoint loader and saver to load and save checkpoints in case the job in interrupted
# and will be restarted by cluster job manager, therefore we want to resume from the last epoch
checkpoint_loader:
  _target_: CheckpointLoader
  _disabled_: $not os.path.exists(@ckpt_path)
  load_path: '@ckpt_path'
  load_dict:
    model: '@network'
    optimizer: '@optimizer'
    logger: '@metriclogger'
    validator: '@evaluator'
    trainer: '@trainer'
    lr_scheduler: '@lr_scheduler'

checkpoint_saver:
  _target_: CheckpointSaver
  save_dir: '@output_dir'
  save_dict:
    model: '@network'
    optimizer: '@optimizer'
    logger: '@metriclogger'
    validator: '@evaluator'
    trainer: '@trainer'
    lr_scheduler: '@lr_scheduler'
  file_prefix: 'ckpt'
  save_interval: '@ckpt_interval'
  save_final: false
  epoch_level: true
  n_saved: 1

##########################
# RUN CONFIGURATION
##########################
run:
- $@checkpoint_loader.attach(@trainer) if @checkpoint_loader is not None else None
- $@checkpoint_saver.attach(@trainer)
- $@val_handlers#0.set_trainer(trainer=@trainer) if @early_stop else None,
- $@trainer.run()